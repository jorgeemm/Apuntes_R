{"title":"Regresión logística","markdown":{"yaml":{"title":"Regresión logística"},"headingText":"Análisis de clasificación","containsRefs":false,"markdown":"\n\n\nLa clasificación supervisada es una tarea muy frecuente en todas las áreas de análisis de datos. Existe un gran número de algoritmos desarrollados tanto por la estadística (regresión logística, análisis discriminante) como por la inteligencia artificial (redes neuronales, árboles de decisión, redes bayesianas) diseñados para realizar las tareas propias de la clasificación.\n\nVamos a centrarnos en el análisis de regresión logística, una técnica para el análisis de variables dependientes categóricas con dos categorías (dicotómicas), o más (polinómicas). Sirve para modelar la probabilidad de ocurrencia de un evento como función de otros factores, y responder preguntas como:\n\n-   ¿Qué factores explican la victoria/derrota de un candidato en unas elecciones?\n\n-   ¿Qué variables determinan que una persona vote?\n\n-   ¿Qué factores incrementan/disminuyen el riesgo de caer en la pobreza?\n\n-   ¿Cómo podemos explicar el abandono escolar?\n\n-   etc\n\nEl análisis de regresión logística pertenece al grupo de *Modelos Lineales Generalizados* (GLM por sus siglas en inglés), y usa como función de enlace la función *logit*.\n\n## Regresión logística vs regresión lineal\n\nEl modelo de regresión lineal no es válido cuando la variable respuesta no tiene una distribución normal. Por ejemplo: respuestas si/no, conteos, probabilidades, etc.\n\nAl igual que la regresión lineal, la regresión logística busca:\n\n-   predecir/explicar una VD a partir de una o mas VI,\n\n-   medir el grado de relación de la VD con las VI\n\n-   comprobar su significatividad\n\nA diferencia de la regresión lineal:\n\n-   los coeficientes de regresión se estiman por el procedimiento de **Máxima Verosimilitud**, que busca maximizar la probabilidad de ocurrencia del evento que se analiza\n\n## Supuestos básicos\n\n**Compartidos** con la Regresión Lineal:\n\n-   Tamaño muestral elevado\n\n-   Introducción de VIs relevantes\n\n-   Variables predictoras continuas o dicotómicas\n\n-   Ausencia de colinealidad entre las VIs\n\n-   Aditividad\n\n**Específicos**:\n\n-   No-linealidad: La función de vinculación logit es no-lineal. Esto implica que **el cambio en la VD producido por el incremento de una unidad en la VI depende del valor que tome la variable**. Es menos importante en los extremos de las VI, y mas importante en los valores centrales.\n\n***COPIAR IMAGEN***\n\n-   Heterocedasticidad: En regresión logística se asume heterocedasticidad (varianza de los residuos no constante). Es lo contrario que en la regresión lineal, ya que la representación de la regresión logística no es lineal, sino que se busca que debe existir varianza en los residuos no constante.\n\n## La ecuación de la regresión logística\n\nLa función de enlace logit, utilizada principalmente en modelos de regresión logística, es como hemos mencionado no lineal, ya que transforma una combinación lineal de predictores en probabilidades mediante la fórmula:\n\n$$\n\\ln\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n$$\n\nEn esta ecuación, la VD aparece en una forma que no es directamente interpretable (concretamente, el logaritmo neperiano de la razón de probabilidades). Haciendo transformaciones, podemos expresar la probabilidad de ocurrencia del suceso de la siguiente manera:\n\n$$\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)}} = \\sigma\\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\right)\n$$\n\ndonde:\n\n$$\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n$$\n\n## Ejercicio de regresión logística\n\nVamos a hacer un sencillo ejercicio de regresión logística usando los datos del la encuesta preelectoral CIS 2023. En concreto, vamos a estimar la probabilidad de que un individuo *i* tenga intención de votar a un parido *p* en las elecciones generales de 2023. Dado que se trata de un ejercicio de clase, vamos a incluir unas pocas variables y no vamos a tomar en consideración los casos perdidos (indecisos, etc). En consecuencia, los resultados no sirven para predicción electoral, solo practicar.\n\nComo de costumbre, abrimos fichero “Limpieza de datos” en su versión más reciente.\n\nA partir de la variable intención de voto (´INTENCIONG\\`), vamos a crear nuestra variable dependiente de intención voto VOX (**intovox**).\n\n```{r message=FALSE, warning=FALSE, include=FALSE}\nsource(\"00_datos/source.r\")\n```\n\n```{r}\n# Intención de voto\ntable(datos$INTENCIONG)\n# Intención voto VOX\ndatos <- datos %>%\n  mutate(\n    intvox = case_when(\n      INTENCIONG == 3 ~ 1,            \n      INTENCIONG >= 9977 ~ NA,  \n      TRUE ~ 0))\n\ntable(datos$intvox, useNA = \"ifany\")\n```\n\nComo variables independientes, vamos a utilizar las variables “edad”, “hombre”, “estudios_universitarios”, “ecoesp”, “ideol” y “recuerdo19” que ya tenemos preparadas de clases anteriores. Una vez tenemos todas las variables preparadas, procedemos a crear el data.frame **data** con el conjunto de variables que vamos a incluir nuestros análisis y eliminamos los casos perdidos.\n\n*Que existan tantos casos perdidos en la variable dependiente (5772) no importa en este caso al ser un ejemplo, pero a la hora de hacer un modelo de predicción real se deberían imputar todos estos valores para que el modelo sea más fiable*.\n\n```{r}\ndatos_log <- datos %>%\n  dplyr::select(intvox, hombre, estudios_universitarios, edad, ecoesp, ideol, recuerdo19) %>%\n  drop_na()\n\nsummary(datos_log)\n```\n\n## Estimación del modelo\n\nYa podemos estimar la regresión logística. De las 4 variables dependientes que tenemos, vamos a empezamos por calcular la probabilidad de votar a VOX frente a otros partidos, en función de la ideología, recuerdo de voto en 2019, opinión sobre la economía en España y perfil socio-demográfico de la persona. Usamos la función `glm()` con *link function* binominal.\n\n```{r}\nlibrary(MASS)\nm.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = datos_log, family = \"binomial\")\nsummary(m.vox)\n```\n\n## Significatividad de las variables\n\nAl igual que en la regresión lineal, contrastamos las siguientes hipótesis:\n\n-   H~0~:βi=0 -\\> la VI~i~ no tiene efecto sobre la VD\n\n-   H~a~:βi!=0 -\\>la VI~i~ sí tiene efecto sobre la VD\n\nComo ya sabemos, podemos rechazar la hipótesis nula siempre que:\n\n-   p-valor\\<0.05, NC:95%\n\n-   p-valor\\<0.01, NC:99%\n\n-   p-valor\\<0.001, NC:99.9%\n\nSi además queremos calcular los intervalos de confianza, podemos usar la función `cofint()`\n\n```{r}\nconfint(m.vox)\n```\n\n## Interpretación de los coeficientes\n\nLos estimadores representan el logaritmo del cociente de probabilidades. Por ejemplo, ceteris paribus:\n\n-   Ideología: Para cada punto que aumenta la ideología, el log de la probabilidad de votar a VOX (versus votar otro partido) aumenta en 0.290559.\n\n-   Votó PP en 2019: el logaritmo de la probabilidad de votar a VOX en 2023 es 0.478263 mayor que entre los que en 2019 votaron al PP que entre los que votaron al PSOE (categoría de referencia).\n\nComo se puede observar, esta interpretación de los coeficientes es muy poco intuitiva. Este tipo de coeficiente es útil si lo que nos interesa es conocer la dirección del efecto (signo positivo o negativo) y el nivel de significación (p-valor). Si por el contrario estamos interesados en interpretar el valor coeficiente, tenemos dos alternativas mejores: expresar los coeficientes como *odds ratio* o calcular las probabilidades de ocurrencia del evento.\n\n## Odds ratio\n\nEl *odds ratio*, o razón de probabilidades se calcula como e^*coef*^ y es la frecuencia de ocurrencia de un suceso sobre la frecuencia de su no ocurrencia:\n\n-   Odds ratio \\>1: la variable tiene un efecto positivo sobre la probabilidad de ocurrencia del suceso.\n\n-   0 \\> Odds ratio \\<1: la variable tiene un efecto negativo sobre la probabilidad de ocurrencia del suceso.\n\n-   Odds ratio =1: la variable no tiene efecto sobre la probabilidad de ocurrencia del suceso.\n\nEn la tabla:\n\n-   Odds ratio~ideología~=e^0.290559^=1.337. Para cada punto que que nos vemos a la derecha en la escala de ideología, la probabilidad de votar a VOX (sobre votar a otro partido) aumenta en un factor de 1.337 (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,337 - 1)\\* 100 = 0,337 \\* 100 = 33,7%\n\n-   Odds ratio~votó PP(2019)~= e^0.478263^=1,61. La probabilidad de votar a VOX (sobre votar a otro partido) es 1,61 veces mayor entre las personas que en 2019 votaron al PP que entre las personas que en 2019 votaron al PSOE (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,61 - 1)\\* 100 = 0,61 \\* 100 = 61%\n\n-   En el caso de que el coeficiente sea negativo, como por ejemplo “opinión sobre la situación de la economía en España” haríamos así: Odds ratio~ecoesp~=e^-1.352050^=0.259. En este caso, la probabilidad de votar a VOX es 0.250 veces menor entre las personas que consideran que la economía nacional va bien, que entre los que consideran que la economía nacional va mal (ceteris paribus). Expresado en porcentaje, sería:\n\n    -   (1 - 0.259)\\* 100 = 0.741 \\* 100 = 74%\n\nPara obtener los coeficientes exponenciados usamos la función `exp()`:\n\n```{r}\nexp(coef(m.vox))\n```\n\nPara ponerlo todo en una tabla, usamos la función `cbind` (*column bind*), que nos permite unir la columna de los coeficientes y la de los intervalos de confianza.\n\n```{r}\nexp(cbind(OR = coef(m.vox), confint(m.vox)))\n```\n\nLos odds ratio se pueden comparar entre sí para saber qué variable es más explicativa o está asociada de manera más fuerte con la VD. Pero OJO! Para comparar un odds ratio mayor que uno (relación positiva) con un odds ratio menor que uno (relación negativa), es necesario calcular el valor inverso de uno de los datos porque el rango es distinto. Por ejemplo:\n\n-   ecoesp: 1/0,259=3,86\n\nLa inversión de variables también resulta útil en el caso de las variables dicotómicas para comprobar el supuesto contrario al establecido. Por ejemplo, la probabilidad de las mujeres (en lugar del de los hombres) es de 1/2,13=0,47.\n\n## Probabilidades predichas\n\nComo alternativa a los coeficientes y a los odds ratio se pueden calcular las probabilidades predichas. Las probabilidades predichas son la mejor manera de entender las variables de un modelo. Para calcularlas, primero debemos crear un *data.frame* con los valores que queremos que tomen las variables independientes en nuestras predicciones.\n\n```{r}\ndata1 <- with(datos_log, data.frame(ideol = mean(ideol), recuerdo19 = c(\"PSOE\",\"PP\",\"VOX\",\"Podemos\",\"Ciudadanos\", \"Más Madrid\", \"Otros\", \"En blanco\"), edad = mean(edad), hombre=\"Hombre\", estudios_universitarios=\"con EU\", ecoesp=\"negativa\"))\n\nhead(data1, 8)\n```\n\nEs importante que las variables en este *data.frame* tengan el mismo nombre que las variables en la regresión logística anterior. Una vez creado el *data.frame*, ya podemos pedirle a R que calcule las probabilidades predichas.\n\n```{r}\ndata1$probpredichas_vox<- predict(m.vox, newdata = data1, type = \"response\")\ndata1[, c(2, 7)]  #le pido que muestre todas las filas de las columnas 2 (recuerdo voto) y 7(probabilidad predicha)\n```\n\nLos resultados muestran la probabilidad predicha de votar a VOX en 2023 para hombres con edad media, ideología media y con estudios universitarios, que opinan que la economía en España va mal, según el partido que habían votado en las elecciones anteriores.\n\n## Efectos marginales\n\nEl paquete *margins* responde a un intento de trasladar el comando “margins” de Stata a R, como un método genérico para calcular los efectos marginales -o efectos parciales- de las variables independientes. Por ejemplo, vamos calcular el efecto marginal de las VIs en nuestro modelo\n\n```{r}\nlibrary(margins)\nmargins_vox <- margins(m.vox)\n\n# Resumen\nsummary_margins <- summary(margins_vox) \nsummary_margins\n```\n\n```{r}\n# Convertimos el resumen en un data.frame para poder hacer un gráfico\ndata_to_plot <- data.frame(\n  factor = summary_margins$factor,\n  AME = summary_margins$AME,\n  lower = summary_margins$lower,\n  upper = summary_margins$upper\n)\n```\n\nTambién podemos representarlos gráficamente. El gráfico a continuación representa la columna AME (*Average Marginal Effect*) y las columnas *low* and *upper* (bandas de confianza inferior y superior)\n\n```{r}\nggplot(data_to_plot, aes(x = AME, y = factor)) +\n  geom_point(color = \"blue\", size = 3) +  # Puntos para los AME\n  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, color = \"black\") +  # Barras de error\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey\") +  # Línea vertical en 0\n  labs(\n    title = \"Average Marginal Effects (AME) with Confidence Intervals\",\n    x = \"AME\",\n    y = \"Variables\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),  # Centrar título\n    axis.text.y = element_text(size = 10)  # Ajustar tamaño de texto\n  )\n\n```\n\nInterpretación: Cuando se dice que un efecto marginal es 0.3882, significa que haber votado a VOX en 2010 está asociado con un aumento promedio de 0.17 unidades en la variable dependiente. En el contexto de una probabilidad o un porcentaje, como suele ser el caso en la regresión logística o modelos similares, un efecto marginal de 0.3882 corresponde a un aumento del 38,82 puntos porcentuales.\n\nPara más información sobre opciones del paquete margins podéis consultar [aquí](https://www.rdocumentation.org/packages/margins/versions/0.3.23) y [aquí](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html).\n\n## Diagnóstico\n\n#### Multicolinealidad\n\n```{r}\nlibrary(rms)\nlogit.vif<- vif(m.vox)\nlogit.vif\n```\n\nTodos los valores están por debajo de 5. No parece que existan problemas de multicolinealidad\n\n#### Heterocedasticidad\n\n```{r}\nlibrary(lmtest)\nlogit.het<-bptest(m.vox)\nlogit.het\n```\n\nLa hipótesis nula en este test es que la varianza de los residuos es constante. La evidencia permite rechazar la hipótesis nula, confirmando que se cumple el supuesto de heterocedasticidad (que es lo que buscamos)\n\n## Presentación de los resultados\n\nVisualizamos los 2 modelos en una misma tabla\n\n```{r}\nlibrary(stargazer)\n\nstargazer(m.vox,\n          type=\"text\",\n          dep.var.labels=c(\"Voto VOX\"),\n          covariate.labels=c(\"Hombre\", \"Estudios Universitarios\", \"Edad\", \"Valoración + economia\", \"Ideología\", \"Voto 2019: PP (cr:PSOE)\", \"Voto 2019: VOX (cr:PSOE)\", \"Voto 2019: Podemos (cr:PSOE)\", \"Voto 2019: Ciudadanos (cr:PSOE)\", \"Voto 2019: +Madrid (cr:PSOE)\", \"Voto 2019: Otros (cr:PSOE)\", \"Voto 2019: blanco (cr:PSOE)\", \"Constante\"))\n```\n\n## Validación del modelos de clasificaicón\n\nA continuación, vamos examinar cómo de bueno/malo es nuestro modelo a la hora de clasificar datos nuevos. Para ello, continuamos con el ejemplo anterior, en el que estimábamos la probabilidad de que un individiuo vote a un determinado partido.\n\nEn primer lugar, creamos el conjunto de entrenamiento (60%) y test (40%). Como hay pocos 1 en la muestra (de la var dependiente), se amplia el % de casos destinados al test para evitar que dentro de este la proporción de 1 sea demasiado bajo como para comprobar el modelo.\n\n```{r}\nset.seed(123)\nindex <- 1:nrow(datos_log)\nporc_test <- 0.40\n# Dividir datos\ntest.data <- datos_log %>% sample_frac(porc_test)  \ntrain.data <- datos_log %>% anti_join(test.data)\n```\n\nCreamos la variable **clase_real**, que corresponde a la variable dependiente (intentación voto a VOX) en el conjunto de test. Es la variable que luego comparemos con los valores estimados para ver nuestro nivel de acierto/error:\n\n```{r}\nclase_real <- test.data$intvox\n```\n\nEntrenamos el modelo (intención de voto a VOX) con los datos del train.data:\n\n```{r}\nlibrary(MASS)\nlogit.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data, family = \"binomial\")\nsummary(logit.vox)\n```\n\nDespués, calculamos los valores predichos en el conjunto de test. Estos son los valores que luego vamos a comparar con la “clase real”:\n\n```{r}\npredicted_logit<- predict(logit.vox, newdata=test.data, type=\"response\")\nhead(predicted_logit)\n```\n\nPara valorar cómo de bien/mal clasifica nuestro modelo, vamos a calcular la curva ROC(*Receiver operating characteristics*) y el AUC (*Area under the curve*). Cuando hacemos una clasificación binaria, existen 4 tipos de resultados posibles:\n\n-   True negative (TN): predecimos 0 y la clase real es 0\n\n-   False negative (FN): predecimos 0 y la clase real es 1\n\n-   True positive (TP): predecimos 1 y la clase real es 1\n\n-   False positive (FP): predecimos 1 y la clase real es 0\n\nA partir de estos resultados se construye la *matriz de confusión*:\n\n![](images/clipboard-2798392842.png){fig-align=\"center\" width=\"400\"}\n\nLa matriz de confusión sive para calcular la **curva ROC**, que es la representación gráfico de la razón de Verdaderos Positivos (TPR) frente a la razón de falsos positivos (FPR):\n\n-   Razón de verdaderos positivos (TPR - true positive rate): proporción de positivos reales que son correctamente identificados como positivos por el modelo. También se le conoce como sensibilidad o recall. Se calcula como:\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\n-   Razón de falsos positivos (FPR - false positive rate) = proporción de negativos reales que son incorrectamente identificados como positivos por el modelo. Se calcula como:\n\n$$\nFPR = \\frac{FP}{FP + TN}\n$$\n\nGráficamente, el espacio ROC se representa de la siguiente manera:\n\n![](images/clipboard-2824448575.png){fig-align=\"center\" width=\"400\"}\n\nLo ideal es encontrar una curva que se acerque lo máximo posible al punto de clasificación perfecta. Si está en la línea, los resultados de la predicción son completamente aleatorios, y si está por debajo de esta el modelo predice aún peor que asignar cifras al azar.\n\nPintamos la curva ROC de nuestro modelo:\n\n```{r}\nlibrary(ROCR)\n\n# Curva ROC\npred_logit <-  prediction(predicted_logit, clase_real) # crea un objeto \"predicción\"\nperf_logit <- performance(pred_logit, measure = \"tpr\", x.measure = \"fpr\") \npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curve\")\n```\n\nY a continuación, calculamos el area bajo la curva (AUC). El AUC (Área Bajo la Curva) es una métrica que mide el desempeño de un modelo de clasificación binaria. Un valor de AUC:\n\n-   1.0: El modelo clasifica perfectamente todas las instancias\n\n-   0.5: El modelo no tiene poder predictivo (es como adivinar al azar)\n\n-   \\< 0.5: El modelo clasifica peor que al azar.\n\n```{r}\nauc.logit<- performance(pred_logit, measure = \"auc\", x.measure = \"fpr\") \nauc.logit@y.values \n```\n\nEn nuestro caso, AUC=0.920. Este valor indica que nuestro modelo clasifica bien en un 92% por de los casos.\n\nNota: auc es un *S4 object system*, por eso las consultas de sus elementos son un poco diferentes a lo que hemos visto hasta ahora. Nosotros no vamos a entrar en esto, pero si teneís curiosidad podéis leer [este capítulo del libro de Hadley Wickham](http://adv-r.had.co.nz/S4.html).\n\n## **Comparación de modelos**\n\nNormalmente, la curva ROC se utiliza para comparar la precisión de diferentes algoritmos de clasificación (como regresión logística, Naive Bayes, Random Forest, LDA, etc). Aunque en nuestro caso solo hemos trabajado con regresión logística, vamos a estimar diferentes modelos a modo de ejemplo, pero sin profundizar en los detalles de su funcionamiento.\n\n### Modelo de clasificación Random Forest\n\nEmpezamos con un [random forest](https://www.r-bloggers.com/2021/04/random-forest-in-r/)\n\n```{r}\nlibrary(randomForest)\n\n# Convertimos la variable intvoto en factor (de lo contrario, da error)\ntrain.data$intvox <- as.factor(train.data$intvox)\ntest.data$intvox <- as.factor(test.data$intvox)\n\n# Entrenamos el modelo Random Forest \nrf.vox <- randomForest(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19,\n  data = train.data,\n  ntree = 500,      # número de árboles\n  mtry = 2,         # número de predictores seleccionados aleatoriamente por árbol\n  importance = TRUE # importancia de variables\n)\n\n# Importancia de las variables\nimportance(rf.vox) \n```\n\nCuanto mayor es el número dentro del Accuracy, más relevante es esa variable dentro de la predicción. Que una variable sea más importante que otra no implica que sea mejor, ya que al no tener ninguna medida como el p-valor no se puede saber si la variable es significativa o no. Por tanto, que una variable sea más importante no la hace relevante en sí.\n\nEn este modelo el type se llama \"prob\" y luego si entre corchetes se pone \\[1\\] clasifica sobre los 0 y \\[,2\\] quiere decir que clasifica sobre los 1.\n\n```{r}\n# Calculamos los valores predichos en el conjunto de test\npredicted_rf <- predict(rf.vox, newdata = test.data, type = \"prob\")[, 2]\n\n# Creamos el objeto de predicción para ROC\npred_rf <- prediction(predicted_rf, clase_real) # valores predichos, valores reales\n\n# Calculamos rendimiento (ROC)\nperf_rf <- performance(pred_rf, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) \nplot(perf_rf, lty = 1, col = \"gold\", main = \"Random Forest ROC Curve\")\n```\n\n```{r}\n# Calculamos el AUC\nauc.rf<- performance(pred_rf, measure = \"auc\", x.measure = \"fpr\") \nauc.rf@y.values\n```\n\nEn este caso, AUC es de 88,7%. Esto indica que en el 87% de los casos, el modelo clasifica correctamente.\n\n### Modelo de clasificación Naive Bayes\n\nRepetimos el proceso con el algoritmo [Naive Bayes](https://www.r-bloggers.com/2021/04/naive-bayes-classification-in-r/)\n\n```{r}\nlibrary(e1071)  \n\n# Entrenamos el modelo Naive Bayes \nnb_model <- naiveBayes(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data)\n\n# Calculamos los valores predichos en el conjunto de test\npredicted_nb <- predict(nb_model, newdata = test.data, type = \"raw\")[, 2] # raw en esta librería equivale a prob en la de random.forest\n\n# Creamos el objeto de predicción para ROC\npred_nb <- prediction(predicted_nb, clase_real) #comparar datos predichos con reales\n\n# Calculamos el rendimiento (ROC)\nperf_nb <- performance(pred_nb, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) # Configuración de un solo gráfico\nplot(perf_nb, lty = 1, col = \"steelblue\", main = \"Naive Bayes ROC Curve\")\n\n```\n\n```{r}\n# Calculamos el AUC\nauc.nb<- performance(pred_nb, measure = \"auc\", x.measure = \"fpr\") \nauc.nb@y.values \n```\n\nEn este caso, el porcentaje de casos correctamente clasificados es 91,6%.\n\nPara tener una visión global vamos a representar todas las curvas ROC de manera conjunta. Esto nos permite comparar los resultados de los tres algoritmos:\n\n```{r}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"ROC Curves\")\nplot(perf_rf, lty=1, col=\"gold\", add = TRUE)\nplot(perf_nb, lty=1, col=\"steelblue\", add = TRUE)\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Random Forest=0.887\", \"Naive Bayes=0.916\"), \n       lty = c(1,1,1),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"gold\",\"steelblue\"),\n       cex = 0.7)\n```\n\nEs importante recordar que nuestros modelos sólo están teniendo en consideración a los individuos que han respondido a todas las preguntas incluidas en el modelo. De esta manera, estamos dejando fuera a muchos entrevistados que no ofrecen información sobre alguna de las variables, particularmente, ideología y recuerdo. Esto genera un importante sesgo. ¿Sería mejor dejar estas variables fuera? A modo de ejemplo, vamos a ver cuál hubiera sido el resultado de no haber incluido información sobre esas variables. Repitamos nuestro modelo original, esta vez excluyendo ideología y recuerdo de voto en el 2019.\n\n```{r}\n# Modelo sin ideología ni recuerdo de voto\nlogit2.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp, data = train.data, family = \"binomial\")\n\n# Predicción\npredicted_logit2<- predict(logit2.vox, newdata=test.data, type=\"response\")\n\n# ROC\npred_logit2 <-  prediction(predicted_logit2, clase_real)\nperf_logit2<- performance(pred_logit2, measure = \"tpr\", x.measure = \"fpr\") \n\n# AUC\nauc.logit2 <- performance(pred_logit2, measure = \"auc\", x.measure = \"fpr\") \nauc.logit2@y.values\n```\n\nLa AUC en este caso es considerablemente más baja. Comparamos ambos modelos gráficamente:\n\n```{r}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curves\")\nplot(perf_logit2, lty=2, col=\"grey\", add = TRUE)\n\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Logit(sin ideología ni recuerdo de voto)=0.765\"), \n       lty = c(1,2),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"grey\"),\n       cex = 0.7)\n```\n\nComo se puede observar en el gráfico arriba, la predicción es mucho peor cuando no disponemos de información sobre ideología y recuerdo de voto. Una alternativa sería imputar los valores perdidos en las variables que tienen un número elevado de NA antes de hacer la regresión.\n\n## **Mejora de modelos**\n\nVamos a explorar qué ocurre si tomamos en consideracion que algunas variables pueden no tener un efecto lineal. Por ejemplo, la edad. Sabemos por otros estudios que el perfil de edad de los votantes de Vox es diverso, pero el partido tiene un respaldo importante entre votantes de mediana edad (35-54 años). Para tomar en cuenta esto, vamos a elevar ambos la variable edad al cuadrado. Recordad que siempre hay que incluir el efecto principal y el cuadrático\n\n```{r}\nlogit3.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + I(edad^2) + ecoesp + ideol + recuerdo19,\n  data = train.data, family = \"binomial\")\nsummary(logit3.vox)\n```\n\nEn nuestro modelo, el efecto principal es positivo, y el cuadrático es negativo. Además, ambos son estadísticamente significativos. La combinación de estos efectos sugiere que la relación entre la variable independiente y la dependiente tiene forma de “U” invertida: La variable dependiente aumenta hasta cierto punto (el máximo) y luego comienza a disminuir.\n\nEn [este post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-the-sign-of-the-quadratic-term-in-a-polynomial-regression/) se explica bastante bien la relación entre signos del efecto principal y cuadrático y la forma del efecto\n","srcMarkdownNoYaml":"\n\n## Análisis de clasificación\n\nLa clasificación supervisada es una tarea muy frecuente en todas las áreas de análisis de datos. Existe un gran número de algoritmos desarrollados tanto por la estadística (regresión logística, análisis discriminante) como por la inteligencia artificial (redes neuronales, árboles de decisión, redes bayesianas) diseñados para realizar las tareas propias de la clasificación.\n\nVamos a centrarnos en el análisis de regresión logística, una técnica para el análisis de variables dependientes categóricas con dos categorías (dicotómicas), o más (polinómicas). Sirve para modelar la probabilidad de ocurrencia de un evento como función de otros factores, y responder preguntas como:\n\n-   ¿Qué factores explican la victoria/derrota de un candidato en unas elecciones?\n\n-   ¿Qué variables determinan que una persona vote?\n\n-   ¿Qué factores incrementan/disminuyen el riesgo de caer en la pobreza?\n\n-   ¿Cómo podemos explicar el abandono escolar?\n\n-   etc\n\nEl análisis de regresión logística pertenece al grupo de *Modelos Lineales Generalizados* (GLM por sus siglas en inglés), y usa como función de enlace la función *logit*.\n\n## Regresión logística vs regresión lineal\n\nEl modelo de regresión lineal no es válido cuando la variable respuesta no tiene una distribución normal. Por ejemplo: respuestas si/no, conteos, probabilidades, etc.\n\nAl igual que la regresión lineal, la regresión logística busca:\n\n-   predecir/explicar una VD a partir de una o mas VI,\n\n-   medir el grado de relación de la VD con las VI\n\n-   comprobar su significatividad\n\nA diferencia de la regresión lineal:\n\n-   los coeficientes de regresión se estiman por el procedimiento de **Máxima Verosimilitud**, que busca maximizar la probabilidad de ocurrencia del evento que se analiza\n\n## Supuestos básicos\n\n**Compartidos** con la Regresión Lineal:\n\n-   Tamaño muestral elevado\n\n-   Introducción de VIs relevantes\n\n-   Variables predictoras continuas o dicotómicas\n\n-   Ausencia de colinealidad entre las VIs\n\n-   Aditividad\n\n**Específicos**:\n\n-   No-linealidad: La función de vinculación logit es no-lineal. Esto implica que **el cambio en la VD producido por el incremento de una unidad en la VI depende del valor que tome la variable**. Es menos importante en los extremos de las VI, y mas importante en los valores centrales.\n\n***COPIAR IMAGEN***\n\n-   Heterocedasticidad: En regresión logística se asume heterocedasticidad (varianza de los residuos no constante). Es lo contrario que en la regresión lineal, ya que la representación de la regresión logística no es lineal, sino que se busca que debe existir varianza en los residuos no constante.\n\n## La ecuación de la regresión logística\n\nLa función de enlace logit, utilizada principalmente en modelos de regresión logística, es como hemos mencionado no lineal, ya que transforma una combinación lineal de predictores en probabilidades mediante la fórmula:\n\n$$\n\\ln\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n$$\n\nEn esta ecuación, la VD aparece en una forma que no es directamente interpretable (concretamente, el logaritmo neperiano de la razón de probabilidades). Haciendo transformaciones, podemos expresar la probabilidad de ocurrencia del suceso de la siguiente manera:\n\n$$\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)}} = \\sigma\\left(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\right)\n$$\n\ndonde:\n\n$$\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n$$\n\n## Ejercicio de regresión logística\n\nVamos a hacer un sencillo ejercicio de regresión logística usando los datos del la encuesta preelectoral CIS 2023. En concreto, vamos a estimar la probabilidad de que un individuo *i* tenga intención de votar a un parido *p* en las elecciones generales de 2023. Dado que se trata de un ejercicio de clase, vamos a incluir unas pocas variables y no vamos a tomar en consideración los casos perdidos (indecisos, etc). En consecuencia, los resultados no sirven para predicción electoral, solo practicar.\n\nComo de costumbre, abrimos fichero “Limpieza de datos” en su versión más reciente.\n\nA partir de la variable intención de voto (´INTENCIONG\\`), vamos a crear nuestra variable dependiente de intención voto VOX (**intovox**).\n\n```{r message=FALSE, warning=FALSE, include=FALSE}\nsource(\"00_datos/source.r\")\n```\n\n```{r}\n# Intención de voto\ntable(datos$INTENCIONG)\n# Intención voto VOX\ndatos <- datos %>%\n  mutate(\n    intvox = case_when(\n      INTENCIONG == 3 ~ 1,            \n      INTENCIONG >= 9977 ~ NA,  \n      TRUE ~ 0))\n\ntable(datos$intvox, useNA = \"ifany\")\n```\n\nComo variables independientes, vamos a utilizar las variables “edad”, “hombre”, “estudios_universitarios”, “ecoesp”, “ideol” y “recuerdo19” que ya tenemos preparadas de clases anteriores. Una vez tenemos todas las variables preparadas, procedemos a crear el data.frame **data** con el conjunto de variables que vamos a incluir nuestros análisis y eliminamos los casos perdidos.\n\n*Que existan tantos casos perdidos en la variable dependiente (5772) no importa en este caso al ser un ejemplo, pero a la hora de hacer un modelo de predicción real se deberían imputar todos estos valores para que el modelo sea más fiable*.\n\n```{r}\ndatos_log <- datos %>%\n  dplyr::select(intvox, hombre, estudios_universitarios, edad, ecoesp, ideol, recuerdo19) %>%\n  drop_na()\n\nsummary(datos_log)\n```\n\n## Estimación del modelo\n\nYa podemos estimar la regresión logística. De las 4 variables dependientes que tenemos, vamos a empezamos por calcular la probabilidad de votar a VOX frente a otros partidos, en función de la ideología, recuerdo de voto en 2019, opinión sobre la economía en España y perfil socio-demográfico de la persona. Usamos la función `glm()` con *link function* binominal.\n\n```{r}\nlibrary(MASS)\nm.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = datos_log, family = \"binomial\")\nsummary(m.vox)\n```\n\n## Significatividad de las variables\n\nAl igual que en la regresión lineal, contrastamos las siguientes hipótesis:\n\n-   H~0~:βi=0 -\\> la VI~i~ no tiene efecto sobre la VD\n\n-   H~a~:βi!=0 -\\>la VI~i~ sí tiene efecto sobre la VD\n\nComo ya sabemos, podemos rechazar la hipótesis nula siempre que:\n\n-   p-valor\\<0.05, NC:95%\n\n-   p-valor\\<0.01, NC:99%\n\n-   p-valor\\<0.001, NC:99.9%\n\nSi además queremos calcular los intervalos de confianza, podemos usar la función `cofint()`\n\n```{r}\nconfint(m.vox)\n```\n\n## Interpretación de los coeficientes\n\nLos estimadores representan el logaritmo del cociente de probabilidades. Por ejemplo, ceteris paribus:\n\n-   Ideología: Para cada punto que aumenta la ideología, el log de la probabilidad de votar a VOX (versus votar otro partido) aumenta en 0.290559.\n\n-   Votó PP en 2019: el logaritmo de la probabilidad de votar a VOX en 2023 es 0.478263 mayor que entre los que en 2019 votaron al PP que entre los que votaron al PSOE (categoría de referencia).\n\nComo se puede observar, esta interpretación de los coeficientes es muy poco intuitiva. Este tipo de coeficiente es útil si lo que nos interesa es conocer la dirección del efecto (signo positivo o negativo) y el nivel de significación (p-valor). Si por el contrario estamos interesados en interpretar el valor coeficiente, tenemos dos alternativas mejores: expresar los coeficientes como *odds ratio* o calcular las probabilidades de ocurrencia del evento.\n\n## Odds ratio\n\nEl *odds ratio*, o razón de probabilidades se calcula como e^*coef*^ y es la frecuencia de ocurrencia de un suceso sobre la frecuencia de su no ocurrencia:\n\n-   Odds ratio \\>1: la variable tiene un efecto positivo sobre la probabilidad de ocurrencia del suceso.\n\n-   0 \\> Odds ratio \\<1: la variable tiene un efecto negativo sobre la probabilidad de ocurrencia del suceso.\n\n-   Odds ratio =1: la variable no tiene efecto sobre la probabilidad de ocurrencia del suceso.\n\nEn la tabla:\n\n-   Odds ratio~ideología~=e^0.290559^=1.337. Para cada punto que que nos vemos a la derecha en la escala de ideología, la probabilidad de votar a VOX (sobre votar a otro partido) aumenta en un factor de 1.337 (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,337 - 1)\\* 100 = 0,337 \\* 100 = 33,7%\n\n-   Odds ratio~votó PP(2019)~= e^0.478263^=1,61. La probabilidad de votar a VOX (sobre votar a otro partido) es 1,61 veces mayor entre las personas que en 2019 votaron al PP que entre las personas que en 2019 votaron al PSOE (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.\n\n    -   (1,61 - 1)\\* 100 = 0,61 \\* 100 = 61%\n\n-   En el caso de que el coeficiente sea negativo, como por ejemplo “opinión sobre la situación de la economía en España” haríamos así: Odds ratio~ecoesp~=e^-1.352050^=0.259. En este caso, la probabilidad de votar a VOX es 0.250 veces menor entre las personas que consideran que la economía nacional va bien, que entre los que consideran que la economía nacional va mal (ceteris paribus). Expresado en porcentaje, sería:\n\n    -   (1 - 0.259)\\* 100 = 0.741 \\* 100 = 74%\n\nPara obtener los coeficientes exponenciados usamos la función `exp()`:\n\n```{r}\nexp(coef(m.vox))\n```\n\nPara ponerlo todo en una tabla, usamos la función `cbind` (*column bind*), que nos permite unir la columna de los coeficientes y la de los intervalos de confianza.\n\n```{r}\nexp(cbind(OR = coef(m.vox), confint(m.vox)))\n```\n\nLos odds ratio se pueden comparar entre sí para saber qué variable es más explicativa o está asociada de manera más fuerte con la VD. Pero OJO! Para comparar un odds ratio mayor que uno (relación positiva) con un odds ratio menor que uno (relación negativa), es necesario calcular el valor inverso de uno de los datos porque el rango es distinto. Por ejemplo:\n\n-   ecoesp: 1/0,259=3,86\n\nLa inversión de variables también resulta útil en el caso de las variables dicotómicas para comprobar el supuesto contrario al establecido. Por ejemplo, la probabilidad de las mujeres (en lugar del de los hombres) es de 1/2,13=0,47.\n\n## Probabilidades predichas\n\nComo alternativa a los coeficientes y a los odds ratio se pueden calcular las probabilidades predichas. Las probabilidades predichas son la mejor manera de entender las variables de un modelo. Para calcularlas, primero debemos crear un *data.frame* con los valores que queremos que tomen las variables independientes en nuestras predicciones.\n\n```{r}\ndata1 <- with(datos_log, data.frame(ideol = mean(ideol), recuerdo19 = c(\"PSOE\",\"PP\",\"VOX\",\"Podemos\",\"Ciudadanos\", \"Más Madrid\", \"Otros\", \"En blanco\"), edad = mean(edad), hombre=\"Hombre\", estudios_universitarios=\"con EU\", ecoesp=\"negativa\"))\n\nhead(data1, 8)\n```\n\nEs importante que las variables en este *data.frame* tengan el mismo nombre que las variables en la regresión logística anterior. Una vez creado el *data.frame*, ya podemos pedirle a R que calcule las probabilidades predichas.\n\n```{r}\ndata1$probpredichas_vox<- predict(m.vox, newdata = data1, type = \"response\")\ndata1[, c(2, 7)]  #le pido que muestre todas las filas de las columnas 2 (recuerdo voto) y 7(probabilidad predicha)\n```\n\nLos resultados muestran la probabilidad predicha de votar a VOX en 2023 para hombres con edad media, ideología media y con estudios universitarios, que opinan que la economía en España va mal, según el partido que habían votado en las elecciones anteriores.\n\n## Efectos marginales\n\nEl paquete *margins* responde a un intento de trasladar el comando “margins” de Stata a R, como un método genérico para calcular los efectos marginales -o efectos parciales- de las variables independientes. Por ejemplo, vamos calcular el efecto marginal de las VIs en nuestro modelo\n\n```{r}\nlibrary(margins)\nmargins_vox <- margins(m.vox)\n\n# Resumen\nsummary_margins <- summary(margins_vox) \nsummary_margins\n```\n\n```{r}\n# Convertimos el resumen en un data.frame para poder hacer un gráfico\ndata_to_plot <- data.frame(\n  factor = summary_margins$factor,\n  AME = summary_margins$AME,\n  lower = summary_margins$lower,\n  upper = summary_margins$upper\n)\n```\n\nTambién podemos representarlos gráficamente. El gráfico a continuación representa la columna AME (*Average Marginal Effect*) y las columnas *low* and *upper* (bandas de confianza inferior y superior)\n\n```{r}\nggplot(data_to_plot, aes(x = AME, y = factor)) +\n  geom_point(color = \"blue\", size = 3) +  # Puntos para los AME\n  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, color = \"black\") +  # Barras de error\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey\") +  # Línea vertical en 0\n  labs(\n    title = \"Average Marginal Effects (AME) with Confidence Intervals\",\n    x = \"AME\",\n    y = \"Variables\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),  # Centrar título\n    axis.text.y = element_text(size = 10)  # Ajustar tamaño de texto\n  )\n\n```\n\nInterpretación: Cuando se dice que un efecto marginal es 0.3882, significa que haber votado a VOX en 2010 está asociado con un aumento promedio de 0.17 unidades en la variable dependiente. En el contexto de una probabilidad o un porcentaje, como suele ser el caso en la regresión logística o modelos similares, un efecto marginal de 0.3882 corresponde a un aumento del 38,82 puntos porcentuales.\n\nPara más información sobre opciones del paquete margins podéis consultar [aquí](https://www.rdocumentation.org/packages/margins/versions/0.3.23) y [aquí](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html).\n\n## Diagnóstico\n\n#### Multicolinealidad\n\n```{r}\nlibrary(rms)\nlogit.vif<- vif(m.vox)\nlogit.vif\n```\n\nTodos los valores están por debajo de 5. No parece que existan problemas de multicolinealidad\n\n#### Heterocedasticidad\n\n```{r}\nlibrary(lmtest)\nlogit.het<-bptest(m.vox)\nlogit.het\n```\n\nLa hipótesis nula en este test es que la varianza de los residuos es constante. La evidencia permite rechazar la hipótesis nula, confirmando que se cumple el supuesto de heterocedasticidad (que es lo que buscamos)\n\n## Presentación de los resultados\n\nVisualizamos los 2 modelos en una misma tabla\n\n```{r}\nlibrary(stargazer)\n\nstargazer(m.vox,\n          type=\"text\",\n          dep.var.labels=c(\"Voto VOX\"),\n          covariate.labels=c(\"Hombre\", \"Estudios Universitarios\", \"Edad\", \"Valoración + economia\", \"Ideología\", \"Voto 2019: PP (cr:PSOE)\", \"Voto 2019: VOX (cr:PSOE)\", \"Voto 2019: Podemos (cr:PSOE)\", \"Voto 2019: Ciudadanos (cr:PSOE)\", \"Voto 2019: +Madrid (cr:PSOE)\", \"Voto 2019: Otros (cr:PSOE)\", \"Voto 2019: blanco (cr:PSOE)\", \"Constante\"))\n```\n\n## Validación del modelos de clasificaicón\n\nA continuación, vamos examinar cómo de bueno/malo es nuestro modelo a la hora de clasificar datos nuevos. Para ello, continuamos con el ejemplo anterior, en el que estimábamos la probabilidad de que un individiuo vote a un determinado partido.\n\nEn primer lugar, creamos el conjunto de entrenamiento (60%) y test (40%). Como hay pocos 1 en la muestra (de la var dependiente), se amplia el % de casos destinados al test para evitar que dentro de este la proporción de 1 sea demasiado bajo como para comprobar el modelo.\n\n```{r}\nset.seed(123)\nindex <- 1:nrow(datos_log)\nporc_test <- 0.40\n# Dividir datos\ntest.data <- datos_log %>% sample_frac(porc_test)  \ntrain.data <- datos_log %>% anti_join(test.data)\n```\n\nCreamos la variable **clase_real**, que corresponde a la variable dependiente (intentación voto a VOX) en el conjunto de test. Es la variable que luego comparemos con los valores estimados para ver nuestro nivel de acierto/error:\n\n```{r}\nclase_real <- test.data$intvox\n```\n\nEntrenamos el modelo (intención de voto a VOX) con los datos del train.data:\n\n```{r}\nlibrary(MASS)\nlogit.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data, family = \"binomial\")\nsummary(logit.vox)\n```\n\nDespués, calculamos los valores predichos en el conjunto de test. Estos son los valores que luego vamos a comparar con la “clase real”:\n\n```{r}\npredicted_logit<- predict(logit.vox, newdata=test.data, type=\"response\")\nhead(predicted_logit)\n```\n\nPara valorar cómo de bien/mal clasifica nuestro modelo, vamos a calcular la curva ROC(*Receiver operating characteristics*) y el AUC (*Area under the curve*). Cuando hacemos una clasificación binaria, existen 4 tipos de resultados posibles:\n\n-   True negative (TN): predecimos 0 y la clase real es 0\n\n-   False negative (FN): predecimos 0 y la clase real es 1\n\n-   True positive (TP): predecimos 1 y la clase real es 1\n\n-   False positive (FP): predecimos 1 y la clase real es 0\n\nA partir de estos resultados se construye la *matriz de confusión*:\n\n![](images/clipboard-2798392842.png){fig-align=\"center\" width=\"400\"}\n\nLa matriz de confusión sive para calcular la **curva ROC**, que es la representación gráfico de la razón de Verdaderos Positivos (TPR) frente a la razón de falsos positivos (FPR):\n\n-   Razón de verdaderos positivos (TPR - true positive rate): proporción de positivos reales que son correctamente identificados como positivos por el modelo. También se le conoce como sensibilidad o recall. Se calcula como:\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\n-   Razón de falsos positivos (FPR - false positive rate) = proporción de negativos reales que son incorrectamente identificados como positivos por el modelo. Se calcula como:\n\n$$\nFPR = \\frac{FP}{FP + TN}\n$$\n\nGráficamente, el espacio ROC se representa de la siguiente manera:\n\n![](images/clipboard-2824448575.png){fig-align=\"center\" width=\"400\"}\n\nLo ideal es encontrar una curva que se acerque lo máximo posible al punto de clasificación perfecta. Si está en la línea, los resultados de la predicción son completamente aleatorios, y si está por debajo de esta el modelo predice aún peor que asignar cifras al azar.\n\nPintamos la curva ROC de nuestro modelo:\n\n```{r}\nlibrary(ROCR)\n\n# Curva ROC\npred_logit <-  prediction(predicted_logit, clase_real) # crea un objeto \"predicción\"\nperf_logit <- performance(pred_logit, measure = \"tpr\", x.measure = \"fpr\") \npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curve\")\n```\n\nY a continuación, calculamos el area bajo la curva (AUC). El AUC (Área Bajo la Curva) es una métrica que mide el desempeño de un modelo de clasificación binaria. Un valor de AUC:\n\n-   1.0: El modelo clasifica perfectamente todas las instancias\n\n-   0.5: El modelo no tiene poder predictivo (es como adivinar al azar)\n\n-   \\< 0.5: El modelo clasifica peor que al azar.\n\n```{r}\nauc.logit<- performance(pred_logit, measure = \"auc\", x.measure = \"fpr\") \nauc.logit@y.values \n```\n\nEn nuestro caso, AUC=0.920. Este valor indica que nuestro modelo clasifica bien en un 92% por de los casos.\n\nNota: auc es un *S4 object system*, por eso las consultas de sus elementos son un poco diferentes a lo que hemos visto hasta ahora. Nosotros no vamos a entrar en esto, pero si teneís curiosidad podéis leer [este capítulo del libro de Hadley Wickham](http://adv-r.had.co.nz/S4.html).\n\n## **Comparación de modelos**\n\nNormalmente, la curva ROC se utiliza para comparar la precisión de diferentes algoritmos de clasificación (como regresión logística, Naive Bayes, Random Forest, LDA, etc). Aunque en nuestro caso solo hemos trabajado con regresión logística, vamos a estimar diferentes modelos a modo de ejemplo, pero sin profundizar en los detalles de su funcionamiento.\n\n### Modelo de clasificación Random Forest\n\nEmpezamos con un [random forest](https://www.r-bloggers.com/2021/04/random-forest-in-r/)\n\n```{r}\nlibrary(randomForest)\n\n# Convertimos la variable intvoto en factor (de lo contrario, da error)\ntrain.data$intvox <- as.factor(train.data$intvox)\ntest.data$intvox <- as.factor(test.data$intvox)\n\n# Entrenamos el modelo Random Forest \nrf.vox <- randomForest(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19,\n  data = train.data,\n  ntree = 500,      # número de árboles\n  mtry = 2,         # número de predictores seleccionados aleatoriamente por árbol\n  importance = TRUE # importancia de variables\n)\n\n# Importancia de las variables\nimportance(rf.vox) \n```\n\nCuanto mayor es el número dentro del Accuracy, más relevante es esa variable dentro de la predicción. Que una variable sea más importante que otra no implica que sea mejor, ya que al no tener ninguna medida como el p-valor no se puede saber si la variable es significativa o no. Por tanto, que una variable sea más importante no la hace relevante en sí.\n\nEn este modelo el type se llama \"prob\" y luego si entre corchetes se pone \\[1\\] clasifica sobre los 0 y \\[,2\\] quiere decir que clasifica sobre los 1.\n\n```{r}\n# Calculamos los valores predichos en el conjunto de test\npredicted_rf <- predict(rf.vox, newdata = test.data, type = \"prob\")[, 2]\n\n# Creamos el objeto de predicción para ROC\npred_rf <- prediction(predicted_rf, clase_real) # valores predichos, valores reales\n\n# Calculamos rendimiento (ROC)\nperf_rf <- performance(pred_rf, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) \nplot(perf_rf, lty = 1, col = \"gold\", main = \"Random Forest ROC Curve\")\n```\n\n```{r}\n# Calculamos el AUC\nauc.rf<- performance(pred_rf, measure = \"auc\", x.measure = \"fpr\") \nauc.rf@y.values\n```\n\nEn este caso, AUC es de 88,7%. Esto indica que en el 87% de los casos, el modelo clasifica correctamente.\n\n### Modelo de clasificación Naive Bayes\n\nRepetimos el proceso con el algoritmo [Naive Bayes](https://www.r-bloggers.com/2021/04/naive-bayes-classification-in-r/)\n\n```{r}\nlibrary(e1071)  \n\n# Entrenamos el modelo Naive Bayes \nnb_model <- naiveBayes(\n  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data)\n\n# Calculamos los valores predichos en el conjunto de test\npredicted_nb <- predict(nb_model, newdata = test.data, type = \"raw\")[, 2] # raw en esta librería equivale a prob en la de random.forest\n\n# Creamos el objeto de predicción para ROC\npred_nb <- prediction(predicted_nb, clase_real) #comparar datos predichos con reales\n\n# Calculamos el rendimiento (ROC)\nperf_nb <- performance(pred_nb, measure = \"tpr\", x.measure = \"fpr\")\n\n# Pintamos la curva ROC\npar(mfrow = c(1,1)) # Configuración de un solo gráfico\nplot(perf_nb, lty = 1, col = \"steelblue\", main = \"Naive Bayes ROC Curve\")\n\n```\n\n```{r}\n# Calculamos el AUC\nauc.nb<- performance(pred_nb, measure = \"auc\", x.measure = \"fpr\") \nauc.nb@y.values \n```\n\nEn este caso, el porcentaje de casos correctamente clasificados es 91,6%.\n\nPara tener una visión global vamos a representar todas las curvas ROC de manera conjunta. Esto nos permite comparar los resultados de los tres algoritmos:\n\n```{r}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"ROC Curves\")\nplot(perf_rf, lty=1, col=\"gold\", add = TRUE)\nplot(perf_nb, lty=1, col=\"steelblue\", add = TRUE)\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Random Forest=0.887\", \"Naive Bayes=0.916\"), \n       lty = c(1,1,1),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"gold\",\"steelblue\"),\n       cex = 0.7)\n```\n\nEs importante recordar que nuestros modelos sólo están teniendo en consideración a los individuos que han respondido a todas las preguntas incluidas en el modelo. De esta manera, estamos dejando fuera a muchos entrevistados que no ofrecen información sobre alguna de las variables, particularmente, ideología y recuerdo. Esto genera un importante sesgo. ¿Sería mejor dejar estas variables fuera? A modo de ejemplo, vamos a ver cuál hubiera sido el resultado de no haber incluido información sobre esas variables. Repitamos nuestro modelo original, esta vez excluyendo ideología y recuerdo de voto en el 2019.\n\n```{r}\n# Modelo sin ideología ni recuerdo de voto\nlogit2.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp, data = train.data, family = \"binomial\")\n\n# Predicción\npredicted_logit2<- predict(logit2.vox, newdata=test.data, type=\"response\")\n\n# ROC\npred_logit2 <-  prediction(predicted_logit2, clase_real)\nperf_logit2<- performance(pred_logit2, measure = \"tpr\", x.measure = \"fpr\") \n\n# AUC\nauc.logit2 <- performance(pred_logit2, measure = \"auc\", x.measure = \"fpr\") \nauc.logit2@y.values\n```\n\nLa AUC en este caso es considerablemente más baja. Comparamos ambos modelos gráficamente:\n\n```{r}\npar(mfrow = c(1,1))\nplot(perf_logit, lty=1, col=\"darkgrey\", main = \"Logit ROC Curves\")\nplot(perf_logit2, lty=2, col=\"grey\", add = TRUE)\n\nlegend(0.4, 0.6,  \n       c(\"Logit=0.920\", \"Logit(sin ideología ni recuerdo de voto)=0.765\"), \n       lty = c(1,2),       \n       bty = \"n\",\n       col=c(\"darkgrey\", \"grey\"),\n       cex = 0.7)\n```\n\nComo se puede observar en el gráfico arriba, la predicción es mucho peor cuando no disponemos de información sobre ideología y recuerdo de voto. Una alternativa sería imputar los valores perdidos en las variables que tienen un número elevado de NA antes de hacer la regresión.\n\n## **Mejora de modelos**\n\nVamos a explorar qué ocurre si tomamos en consideracion que algunas variables pueden no tener un efecto lineal. Por ejemplo, la edad. Sabemos por otros estudios que el perfil de edad de los votantes de Vox es diverso, pero el partido tiene un respaldo importante entre votantes de mediana edad (35-54 años). Para tomar en cuenta esto, vamos a elevar ambos la variable edad al cuadrado. Recordad que siempre hay que incluir el efecto principal y el cuadrático\n\n```{r}\nlogit3.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + I(edad^2) + ecoesp + ideol + recuerdo19,\n  data = train.data, family = \"binomial\")\nsummary(logit3.vox)\n```\n\nEn nuestro modelo, el efecto principal es positivo, y el cuadrático es negativo. Además, ambos son estadísticamente significativos. La combinación de estos efectos sugiere que la relación entre la variable independiente y la dependiente tiene forma de “U” invertida: La variable dependiente aumenta hasta cierto punto (el máximo) y luego comienza a disminuir.\n\nEn [este post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-the-sign-of-the-quadratic-term-in-a-polynomial-regression/) se explica bastante bien la relación entre signos del efecto principal y cuadrático y la forma del efecto\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"include-in-header":{"text":"<style>\n/* Estilos para todo el documento */\nbody {\n  text-align: justify;\n}\n\n/* Estilos específicos para la tabla de contenidos */\n.toc-actions, .toc .nav, .toc .nav > li > a {\n  text-align: left !important;\n}\n</style>\n"},"output-file":"54_reg_logistica.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","knitr":{"opts_chunk":{"echo":true,"message":false,"warning":false,"fig.align":"center","out.width":"60%"}},"editor":"visual","theme":{"light":"flatly","dark":"darkly"},"toc-title":"Contenido","title":"Regresión logística"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}